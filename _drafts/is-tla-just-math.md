---
layout: post
title:  "TLA isn't \"just ordinary math\""
comments: true
tag: TLA+
tag: formal verification
---

This is a rant on the "TLA is great because it's just ordinary math" claim made by Leslie Lamport, that I have seen parroted often enough to annoy me. I actually have no gripes with the first part of the sentence -- I do think that Temporal Logic of Actions (TLA) *is* great, but I don't think it's because it's "ordinary math" and I want to explain why.

The chain of arguments is roughly the following:

1. Ordinary math is great at rigorously describing abstract concepts.
2. When designing algorithms and systems, the designs are abstract, so one should use ordinary math to rigorously describe them.
3. Ordinary mathematicians use first-order logic (FOL), and in particular Zermelo-Frankel (ZF) set theory, which is a theory in FOL.
4. TLA predicates are based on first-order set theory. Hence TLA is great for describing algorithms and system designs, since it's just ordinary math.

The first claim here that I disagree with is #3. While "ordinary math" can indeed be formalized in ZF (or ZFC, ZF with choice), most "ordinary mathematicians" are not logicians nor set theorists. I think my undergrad in math reflects most people's experience. You're introduced to ZF sometimes during your undergrad. You're told that you can sleep well since math's strong and rigorous logical foundations provided by ZF. You get to see the axioms of ZF, you get to be frightened by the encoding of natural numbers in ZF[^encoding], and you get to be amused by the Banach-Tarski paradox. But you mostly go on to forget about it and never touch it again, and do stuff in analysis, algebra, discrete math, topology, probability, or what have you. You silently sneer at the logicians, who aren't doing any real math anyways.

Now that's not to say that a mathematician doesn't often write down FOL formulas -- they do. But, in my second-hand experience of observing them throughout the years, they will much more often write things like "suppose that", "by contradiction", "without loss of generality", "analogously", or the venerable "it can easily be seen that". Good luck writing that last one down in FOL. So I claim that claim #3 is bogus.

A "working mathematician" generally operates at a much higher level of abstraction, and a lower level of formality and rigor than FOL. Or for that matter, HOL (higher-order logic), Martin-LÃ¶f's type theory, or anything other kind of formalism; any of these will tend to feel like a bit of a straitjacket when writing down proofs. In that sense, mathematicians like to "move fast and break things" (I didn't really expect to ever say that, but there, I said it). Now there are downsides to this, as new math proofs can hang in the limbo for years, because nobody can confidently judge their truth. Ask Grigori Perelman, whose proof of the Poincare conjecture took years to be accepted. Possibly worse is the story I heard at a lecture by the late Vladimir Voevodsky[^Voevodsky]. He recounted his experience where he had published a wrong result, which later got correctly challenged, but he incorrectly believed the challenger to be wrong, and proceeded to publish a rebuttal of the challenge, only to realize that he was wrong all along years later. And we're talking about a Fields medalist here.

Anyway. I'd like to continue my rant by attacking the claim #2 next. I think that using ordinary math to describe computer systems is suboptimal. My exhibit A is a [complicated system](https://canton.network) that I was working on. It provides atomic transactions across data held by mutually mistrusting parties, with each party having only partial visibility into the transaction (so you could, for example, atomically trade securities for money, without the central securities depository knowing the price at which they were traded, nor the settlement bank knowing what was bought). The design was complicated, but I managed to tease out a compact and elegant description of the main security property, the wet dream of a formal verification specialist. The proof on the other hand was more of a nightmare. The whole design doc spread over something like 80 pages, with ordinary math style definitions, theorems and lemmas carefully orchestrating the system's security. The whole ensemble would then start squeaking and creaking any time we'd change something in the design in order to improve any properties, simplify the implementation, or add new features. For each definition we'd write down, we'd need to think whether and how it has been affected by the change. And for each changed definition, we had to meticulously sift through all the proofs and make sure that they still hold, or adapt them. If it wasn't for colleagues with a higher level of discipline (and likely an even higher pain threshold) than yours truly who'd make me painstakingly go through the proofs, we would've missed problems for sure. And I can't be quite certain we hadn't missed any anyways.

So I actually think that ordinary math isn't that great at handling large proofs. I think that programmers are in some sense better equipped at that, actually. Because we have, things like, you know, [sane](https://en.wikipedia.org/wiki/Standard_ML#Module_system) or even [great](https://people.mpi-sws.org/~rossberg/papers/Rossberg%20-%201ML%20--%20Core%20and%20modules%20united%20%5BJFP%5D.pdf) module systems, that can organize large developments well. Well some of us get to have those, at least. But perhaps even more importantly, I think that ordinary math is kind of terrible at proof maintenance. Maintenance is perhaps less of a problem in math; you will be tweaking your definitions and redo your proofs while you're writing a paper, but after that the result is generally done. But computer systems are always under development, and you'll need to change and refactor the proofs as you're doing the same with the code. When it comes to refactoring, I think that there is something to be learned from programmers, because we're refactoring all the frickin time.

And I find refactoring Haskell, Rust or Scala code a *lot* more pleasant than refactoring Python code.
Which leads me to one thing that I actually don't like about TLA+ (the diatribe continues). Which is the choice of untyped (aka "unsorted") FOL for predicates. The lack of types in my experience makes refactoring TLA models more painful than necessary. It's still not as bad as Python, because TLA code will likely end up being analyzed the TLA model checker, which will eventually point out your refactoring errors. But it can take minutes, hours, or days until you get that feedback. A sensible type checker would catch that immediately.

Now in TLA's defense, this wasn't really a concern back when it was designed. Lamport designed it to be written using pen and paper, and not as a computer language. But things have evolved since then, and TLA has tool support. Now there are of course also downsides to types; there's actually a [reasonably interesting paper](https://dl.acm.org/doi/pdf/10.1145/319301.319317) where Lamport and the author of Isabelle/HOL, Lawrence Paulson, discuss the merits of types in specification languages. However, in my experience models are not much different from programs. For small programs, I'll often reach out for Python, as it's easy enough to keep everything in your head without having to worry about types (at least not static types). But for anything larger I'll prefer something with a sensible type system (modulo other practical concerns, such as libraries).

So since I'm whining about TLA so much, why have I been using TLA to write a [bunch of models](https://github.com/dfinity/tla-models) for work the last couple of years? Because there's a bunch of things that make it great:

1. It's an expressive language: first-order set theory, but extended with higher-order operators, yet it comes with a model checker. So it's easy to write models of many systems, yet you can still throw a tool at them. This is very different to model checking systems such as SPIN/Promela, or NuSMV; their heavily restricted languages make it a pain to write many things down.
1. Its main model checker (TLC) is explicit-state, meaning that it will just exhaustively enumerate all states of your system. While this is highly inefficient, it's predictably inefficient, unlike other methods involving binary decision diagrams or SMT solvers, which all contain dark magic.
1. It's a small and inextensible language, meaning that a colleague can review my models without having to spend days learning TLA. To contradict myself a bit, not having a type system does contribute to keeping the language small. Now this is a tradeoff again in other respects; no types and no syntactic extensions can make it painful to write large models. But you're likely to hit the limits of TLC before your models get large enough for this to matter.
1. I think the "T" in TLA (temporal) is actually quite clever and nice. While it's hard to untangle this from the choice of first-order logic as opposed to predicate logic, I think it's probably more ergonomic than the usual linear temporal logic (LTL).

But "it's just ordinary math" is not in this list.

### Footnotes

[^encoding]: There are [different ways](https://en.wikipedia.org/wiki/Set-theoretic_definition_of_natural_numbers) to encode natural numbers in ZF. For example, with the Von Neumann encoding, you encode `0` as the empty set, and the successor of the number `N` is encoded as {% raw %}`Encoding(Suc(N)) = {{}, Encoding(N)}`{% endraw %}.
[^Voevodsky]: Voevodsky also describes the experience [here](https://www.ias.edu/sites/default/files/pdfs/publications/letter-2014-summer.pdf)
[^sane-modules]:
